{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = np.load(\"Data/train_rois4.npy\", encoding='latin1')[:,1]\n",
    "test_data = np.load(\"Data/test_rois4.npy\", encoding='latin1')[:,1]\n",
    "train_labels = pd.read_csv(\"Data/train_labels.csv\")\n",
    "labels = np.array(train_labels)[:,1]\n",
    "labels = LabelEncoder().fit_transform(labels)\n",
    "train_data = np.array([train_data[i].reshape(1,28,28) for i in range(len(train_data))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg= LogisticRegression(class_weight = \"balanced\", multi_class='multinomial', solver='saga')\n",
    "nb = GaussianNB()\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(100), learning_rate_init = 0.01, learning_rate = 'adaptive')\n",
    "\n",
    "nb.fit(Xtrain,ytrain)\n",
    "mlp.fit(Xtrain,ytrain)\n",
    "logreg.fit(Xtrain,ytrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Naive bayes predictions\n",
    "nb_preds = nb.predict(Xvalid)\n",
    "num_correct_nb= np.sum(nb_preds == yvalid)\n",
    "\n",
    "#MLP Classifier predictions\n",
    "mlp_preds = mlp.predict(Xvalid)\n",
    "num_correct_mlp= np.sum(mlp_preds == yvalid)\n",
    "\n",
    "#Logistic Regression predictions\n",
    "logreg_preds = logreg.predict(Xvalid)\n",
    "num_correct_logreg = np.sum(logreg_preds == yvalid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Naive bayes accuracy: \", num_correct_nb/len(yvalid) * 100, \"%\")\n",
    "print(\"Logistic Regression accuracy: \", num_correct_logreg/len(yvalid) * 100, \"%\")\n",
    "print(\"MLP accuracy: \", num_correct_mlp/len(yvalid) * 100, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "%matplotlib inline\n",
    "import random\n",
    "import cnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuickDrawDataset(Dataset):\n",
    "\n",
    "    def __init__(self, data_path, label_path, transform=None):\n",
    "        self.data =  np.load(data_path, encoding='latin1')[:,1]\n",
    "        train_labels =  pd.read_csv(label_path)\n",
    "        train_labels = np.array(train_labels)[:,1]\n",
    "        self.labels = LabelEncoder().fit_transform(train_labels)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx].reshape(1,28,28)\n",
    "        target = self.labels[idx]\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample,target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = QuickDrawDataset(\"Data/train_rois4.npy\",\"Data/train_labels.csv\" )\n",
    "batch_size = 64\n",
    "batch_size_eval = 512\n",
    "n_valid = 1000\n",
    "indices = list(range(len(train_data)))\n",
    "random.shuffle(indices)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_data,\n",
    "    batch_size=batch_size,\n",
    "    sampler=SubsetRandomSampler(indices[n_valid:])\n",
    ")\n",
    "\n",
    "valid_loader = DataLoader(\n",
    "    train_data,\n",
    "    batch_size=batch_size_eval,\n",
    "    sampler=SubsetRandomSampler(indices[:n_valid])\n",
    ")\n",
    "\n",
    "for inputs,targets in train_loader:\n",
    "    print(\"This is the shape of one batch\" ,inputs.shape)\n",
    "    print(\"target\", targets.shape)\n",
    "    img = inputs[0,0]\n",
    "    plt.imshow(img, cmap='Greys_r')\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If a GPU is available, use it\n",
    "# Pytorch uses an elegant way to keep the code device agnostic\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    use_cuda = True\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    use_cuda = False\n",
    "    \n",
    "print(device)\n",
    "\n",
    "def train(model,train_loader, optimizer, epoch ):\n",
    "    \"\"\"Perform one epoch of training.\"\"\"\n",
    "    model.train()\n",
    "    \n",
    "    for batch_idx, (inputs, target) in enumerate(train_loader):\n",
    "        inputs, target = inputs.to(device), target.to(device)\n",
    "        \n",
    "        # Let them code what's here\n",
    "        optimizer.zero_grad()\n",
    "        output = model(inputs)\n",
    "        loss = loss_fn(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        ###\n",
    "        \n",
    "        if batch_idx % 30 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(inputs), len(train_loader) *len(inputs) ,\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "def test(model, test_loader):\n",
    "    \"\"\"Evaluate the model by doing one pass over a dataset\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    test_size = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, target in test_loader:\n",
    "            inputs, target = inputs.to(device), target.to(device)\n",
    "            \n",
    "            # let them code what's here\n",
    "            output = model(inputs)\n",
    "            test_size += len(inputs)\n",
    "            test_loss += test_loss_fn(output, target).item() # sum up batch loss\n",
    "            pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= test_size\n",
    "    accuracy = correct / test_size\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, test_size,\n",
    "        100. * accuracy))\n",
    "    \n",
    "    return test_loss, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = cnn.KaggleNet()\n",
    "model = model.double()\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "test_loss_fn = nn.CrossEntropyLoss(reduction='sum')\n",
    "lr = 0.001\n",
    "optimizer = optim.Adam(model.parameters(), lr = lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "savedir = \"logs/\"\n",
    "\n",
    "results = {'name':'trainlog', 'lr': lr, 'loss': [], 'accuracy':[]}\n",
    "savefile = os.path.join(savedir, results['name']+str(results['lr'])+'.pkl' )\n",
    "\n",
    "for epoch in range(1, 10):\n",
    "    train(model, train_loader, optimizer, epoch)\n",
    "    loss, acc = test(model, valid_loader)\n",
    "    \n",
    "    # save results every epoch\n",
    "    results['loss'].append(loss)\n",
    "    results['accuracy'].append(acc)\n",
    "    with open(savefile, 'wb') as fout:\n",
    "        pickle.dump(results, fout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Or just use skorch!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skorch import NeuralNetClassifier\n",
    "from skorch.dataset import CVSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If a GPU is available, use it\n",
    "# Pytorch uses an elegant way to keep the code device agnostic\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    use_cuda = True\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    use_cuda = False\n",
    "net = NeuralNetClassifier(module=cnn.KaggleNet().double(), criterion=nn.CrossEntropyLoss, optimizer = optim.Adam, \n",
    "                          lr= 0.005, max_epochs = 20, train_split = CVSplit(10, random_state = 0)\n",
    "                          , device = device, iterator_train__batch_size = 64, iterator_valid__batch_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss    valid_acc    valid_loss      dur\n",
      "-------  ------------  -----------  ------------  -------\n",
      "      1        \u001b[36m1.6672\u001b[0m       \u001b[32m0.5890\u001b[0m        \u001b[35m1.3826\u001b[0m  78.9072\n",
      "      2        \u001b[36m1.0655\u001b[0m       \u001b[32m0.6250\u001b[0m        \u001b[35m1.2459\u001b[0m  78.0133\n",
      "      3        \u001b[36m0.8492\u001b[0m       0.6150        1.4477  77.7370\n",
      "      4        \u001b[36m0.6598\u001b[0m       \u001b[32m0.6570\u001b[0m        1.2491  78.1190\n",
      "      5        \u001b[36m0.5063\u001b[0m       0.6430        1.3190  77.8305\n",
      "      6        \u001b[36m0.3966\u001b[0m       0.6530        1.3237  77.5771\n",
      "      7        \u001b[36m0.2934\u001b[0m       \u001b[32m0.6870\u001b[0m        1.2624  77.3298\n",
      "      8        \u001b[36m0.2206\u001b[0m       0.6870        1.3050  77.8745\n",
      "      9        \u001b[36m0.1837\u001b[0m       \u001b[32m0.7140\u001b[0m        1.2897  78.8375\n",
      "     10        \u001b[36m0.1511\u001b[0m       0.7030        1.3473  79.5987\n",
      "     11        \u001b[36m0.1184\u001b[0m       0.6980        1.3706  79.8653\n",
      "     12        \u001b[36m0.0905\u001b[0m       0.7140        1.3208  81.4103\n",
      "     13        \u001b[36m0.0898\u001b[0m       0.6990        1.4168  77.8051\n",
      "     14        \u001b[36m0.0783\u001b[0m       0.7120        1.4719  77.4083\n",
      "     15        \u001b[36m0.0700\u001b[0m       0.7110        1.5620  77.6334\n",
      "     16        \u001b[36m0.0669\u001b[0m       \u001b[32m0.7190\u001b[0m        1.5360  77.5741\n",
      "     17        \u001b[36m0.0582\u001b[0m       0.7080        1.5693  77.6237\n",
      "     18        \u001b[36m0.0532\u001b[0m       0.6920        1.6367  77.1537\n",
      "     19        \u001b[36m0.0504\u001b[0m       \u001b[32m0.7220\u001b[0m        1.4366  78.3017\n",
      "     20        \u001b[36m0.0361\u001b[0m       0.7190        1.5090  77.6978\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<class 'skorch.classifier.NeuralNetClassifier'>[initialized](\n",
       "  module_=KaggleNet(\n",
       "    (conv1): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (conv3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (bn3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (dropout1): Dropout(p=0.2)\n",
       "    (fc1): Linear(in_features=3136, out_features=512, bias=True)\n",
       "    (bn4): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (dropout2): Dropout(p=0.2)\n",
       "    (fc2): Linear(in_features=512, out_features=31, bias=True)\n",
       "  ),\n",
       ")"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.fit(train_data,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# saving\n",
    "with open('model.pkl', 'wb') as f:\n",
    "    pickle.dump(net, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:fondements_ml]",
   "language": "python",
   "name": "conda-env-fondements_ml-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
